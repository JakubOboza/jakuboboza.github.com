<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: redis | No Fucking Idea]]></title>
  <link href="http://JakubOboza.github.com/blog/categories/redis/atom.xml" rel="self"/>
  <link href="http://JakubOboza.github.com/"/>
  <updated>2012-08-01T12:43:52+01:00</updated>
  <id>http://JakubOboza.github.com/</id>
  <author>
    <name><![CDATA[Jakub Oboza]]></name>
    <email><![CDATA[jakub.oboza@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Designing Twitter clone in redis]]></title>
    <link href="http://JakubOboza.github.com/blog/2012/07/31/designing-twitter-clone-in-redis/"/>
    <updated>2012-07-31T14:02:00+01:00</updated>
    <id>http://JakubOboza.github.com/blog/2012/07/31/designing-twitter-clone-in-redis</id>
    <content type="html"><![CDATA[<p>Ont he official redis site <a href="http://redis.io">http://redis.io</a>  you can find this <a href="http://redis.io/topics/twitter-clone/">http://redis.io/topics/twitter-clone/</a> post about building twitter clone in redis. I based my design post partially on it but i would like to go more deep into building timeline and posts.</p>

<h1>Quick review</h1>

<p>I used similar approach to store followers and following so i will just go fast through the keys and design.</p>

<p><code>bash
  twtr:&lt;user_id&gt;:follows -&gt; set of ids this user follows
  twtr:&lt;user_id&gt;:followers -&gt; set of id's  that follows this user
</code></p>

<p>What happens when i click "follow"
<code>bash example
redis 127.0.0.1:6379&gt; SADD twtr:kuba:following amelia
(integer) 1
redis 127.0.0.1:6379&gt; SADD twtr:amelia:followers kuba
(integer) 1
redis 127.0.0.1:6379&gt; SADD twtr:kuba:following dan
(integer) 1
redis 127.0.0.1:6379&gt; SADD twtr:dan:followers kuba
(integer) 1
redis 127.0.0.1:6379&gt; SADD twtr:kuba:following ben
(integer) 1
redis 127.0.0.1:6379&gt; SADD twtr:ben:followers kuba
(integer) 1
redis 127.0.0.1:6379&gt; SMEMBERS twtr:kuba:following
1) "ben"
2) "amelia"
3) "dan"
</code>
So now i follow amelia, ben and dan.</p>

<p>What happens when amelia click followers!
<code>bash example
redis 127.0.0.1:6379&gt; SMEMBERS twtr:amelia:followers
1) "kuba"
</code></p>

<p>This way for each user we can see set of people who follow him and those who he follows. Thats all we need like in tutorial.</p>

<h1>Post</h1>

<p>So i think it is not waste if we will decide to keep post in form of 2 keys</p>

<p><code>bash
  twtr:&lt;user_id&gt;:post:&lt;post_id&gt; -&gt; content text of post
  twtr:&lt;user_id&gt;:post:&lt;post_id&gt;:created_at -&gt; creation time of post
  twtr:post_owner:&lt;post_id&gt; -&gt; id of post creator.
</code>
Why this approach and not compacting all the things into pipe separate key? Both solutions seems to be ok, this just leaves little bit more flexibility. I know that it will generate 2 x times more pickups to redis then previous so you can consider doing</p>

<p><code>bash
  twtr:&lt;user_id&gt;:post:&lt;post_id&gt; -&gt; (timestamp|text)
</code></p>

<p>Both solutions have a pros and cones, first one will require 2 x lookups and second one will require parsing data in app layer. Still i prefer first one.</p>

<h2>Post id</h2>

<p>This is hard topic, because in future we would want to scale ( lol ) . Generating post id is not easy task in this case. We could just use auto incrementing counter like this</p>

<p><code>bash
  twtr:post_id:next -&gt; auto incr
</code></p>

<p>But to have something more flexible you should look at something like snowflake <a href="http://engineering.twitter.com/2010/06/announcing-snowflake.html">http://engineering.twitter.com/2010/06/announcing-snowflake.html</a>.</p>

<h1>Post list</h1>

<p>For each user we will store a list of posts he wrote. Initially i thought that we could just pump everything into list. But this is not optimal. This is single point which will grow like crazy and we will be not able to decide how and when to archive parts that are not used. Eg. posts did by user 8 months ago are not really relevant today because if we will make assumption that on average person posts few times a week this 8 month old entry will be way forgotten. We want to archive it, also it will be healthier for memory to store short lists.</p>

<p>I see here two scenarios:</p>

<ul>
<li>user looks at his last few posts &lt; 100</li>
<li>user is infinite scrolling through all posts.</li>
</ul>


<p>So this scenario reasonable seems to have list of lists in which we will have ordered post lists id's. if we will use only LPUSH to add posts lists tot his list we will be able to to do easy LRANGE 0 <range> to get newest lists.</p>

<p><code>bash
  twtr:&lt;user_id&gt;:lists -&gt; list of lits id's only LPUSH id and LRANGE 0 number.
  twtr:&lt;user_id&gt;:list_next -&gt; auto incr counter for lists id's
  twtr:&lt;user_id&gt;:list:&lt;list_id&gt; -&gt; list with 100 posts
</code>
So how do we get most recent posts ? we just  LRANGE 0 2 to get most recent two lists and next we will merge them first + second. Both are LPUSH'ed so should be semi ordered. (we don't really care about order). adding stuff to time line is bit tricky.</p>

<p>we need to do it like this
<code>LRANGE &lt;key&gt; 0 1</code> current list id, next we need to <code>LLEN &lt;key&gt;</code> to check size and if it is &lt; SIZE ( for our example 100 ) we just <code>LPUSH &lt;key&gt; &lt;value&gt;</code> and job done, if size is > 100 we need to <code>INCR &lt;list counter&gt;</code> and LPUSH its result on list of lists and next we need to <code>LPUSH &lt;key&gt; &lt;value&gt;</code> on the new list.</p>

<p>And all of this in application layer. But this is the hardest bit to do. May seem to be complicated but if this seems to be not optimal you can add one more list</p>

<p><code>bash
  twtr:&lt;user_id&gt;:list:current -&gt; list of current 100 posts
</code>
This list is just the most current posts of particular user. How does this list work ? Algorithm is simple</p>

<ul>
<li>LPUSH new post id's</li>
<li>RPOP if SIZE > 100</li>
</ul>


<p>This could be useful to reduce number of hits you get against redis.</p>

<h2>Time line</h2>

<p>Now the time line. Time line is exactly the same as user post list. we will just one "bit" about adding posts.</p>

<p>Algorithm here is when you add a post you have to pick all id's of people who follow you. (example from top if you are adding post as amelia)
<code>bash
SMEMBERS twtr:&lt;user_id&gt;:followers
</code>
And you need to push your post id into their time line posts list. Thats all. Ofc one thing that we need to add are keys for time line</p>

<p><code>bash
  twtr:&lt;user_id&gt;:timeline:lists -&gt; list of lits id's only LPUSH id and LRANGE 0 number.
  twtr:&lt;user_id&gt;:timeline:list_next -&gt; auto incr counter for lists id's
  twtr:&lt;user_id&gt;:timeline:list:&lt;list_id&gt; -&gt; list with 100 posts
  twtr:&lt;user_id&gt;:timeline:current -&gt; LPUSH, RPOP &gt; SIZE current list cache
</code></p>

<h1>Summary</h1>

<p>This is how i would approach building twitter like clone. Things like old lists can be easily archived into mysql, postgres or other thing and EXPIREed from redis.  One thing in my design is common that i put a lot into keys <user_id> this could be skiped but in my opinion it is not bad. IF you will use <user_id> in form of user email md5 you can use it directly to access gravatar of that user.</p>

<p>On average you will need to do around 10-30 hits into redis to get data if you plan to do it in a "lazy way" you can minimize number of hits to around 10.</p>

<p>If you see problem with my design comment i want to know it!.  The core of this design is that each user post data is stored into one redis instance. This is important because of access and race condition stories if you will have many redis instances. But achieving "sharding" in application layer is not hard. Only thing that i would care about is post id generator. This is single point of failure because i have a strong assertion that post_id is unique in whole system.</p>

<p>Cheers!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Designing todo app backend using redis and mongodb]]></title>
    <link href="http://JakubOboza.github.com/blog/2012/07/26/designing-todo-app-backend-using-redis-and-mongodb/"/>
    <updated>2012-07-26T18:06:00+01:00</updated>
    <id>http://JakubOboza.github.com/blog/2012/07/26/designing-todo-app-backend-using-redis-and-mongodb</id>
    <content type="html"><![CDATA[<h2>side note</h2>

<p>Upgrading octopress is a @!#!@pain! :></p>

<h1>What is this post about :)</h1>

<p>Long time nothing new here so i will glue something together about stuff that we were talking about today with my friend Jarek. We talked about building backend for Todo app :). Yes simple todo app. How to build scalable backend. So my initial thought was "how i would design it in different databases". (i'm taking only about data model)</p>

<h2>Requirements</h2>

<p>What we know:</p>

<ul>
<li>User has some sort of id. (number, email, hash of something)</li>
<li>We need to be able to have different todo lists</li>
<li>User can choose his todo list and see tasks ( obvious )</li>
<li>User can tag tasks!</li>
<li>User can query tasks in list by tags</li>
<li>User can see all tags.</li>
</ul>


<h1>Design using Redis</h1>

<p>How to do it with redis ? :)</p>

<p>First few facts i assumed at start. Single todo task has body and timestamps [created_at, updated_at] and base for key will be phrase "todoapp".</p>

<p>So lets start with user and his list of todo lists :). This gives us first key</p>

<p><code>bash
todo:&lt;user_id&gt;:todolist:next =&gt; auto incrementing counter for lists id
todo:&lt;user_id&gt;:todolists =&gt; [LIST]
todo:&lt;user_id&gt;:todolist:&lt;todo_list_id&gt;:name =&gt; list name
</code></p>

<p>Here we have two keys, first is  list id counter that we will bump to get new list counter :), second is list of todolists ids. Why do it this way ? Well people can add and remove todo lists.</p>

<p>Ok so how to create new list ?</p>

<p><code>bash example
redis 127.0.0.1:6379&gt; INCR todo:kuba:todolist:next
(integer) 1
redis 127.0.0.1:6379&gt; RPUSH todo:kuba:todolists 1
(integer) 1
redis 127.0.0.1:6379&gt; LRANGE todo:kuba:todolists 0 -1
1) "1"
redis 127.0.0.1:6379&gt; SET todo:kuba:todolist:1:name "things to do"
OK
</code>
Hey ! we just added id of our first list to list of our todo lists (lots of list word here!). Ok so now lets add a task.</p>

<p>list:
<code>bash
todo:&lt;user_id&gt;:todolist:&lt;todo_list_id&gt;:next =&gt; auto incrementing counter for tasks id
todo:&lt;user_id&gt;:todolist:&lt;todo_list_id&gt; =&gt; [LIST]
</code>
and task:
<code>bash
todo:&lt;user_id&gt;:task:&lt;task_id&gt; =&gt; content of task eg. "finish blog post"
todo:&lt;user_id&gt;:task:&lt;task_id&gt;:created_at =&gt; epoch time when it was created handled by app
todo:&lt;user_id&gt;:task:&lt;task_id&gt;:updated_at =&gt; epoch time when it was last updated handled by app
</code></p>

<p>Ok so how to i add task to my list
<code>bash adding task
redis 127.0.0.1:6379&gt; INCR todo:kuba:todolist:1:next
(integer) 1
redis 127.0.0.1:6379&gt; LPUSH todo:kuba:todolist:1 1
(integer) 1
redis 127.0.0.1:6379&gt; SET todo:kuba:task:1 "finish blog post"
OK
redis 127.0.0.1:6379&gt; SET todo:kuba:task:1:created_at  1343324314
OK
redis 127.0.0.1:6379&gt; SET todo:kuba:task:1:updated_at  1343324315
OK
</code>
And we have our first task in. How do we get tasks from out todo list simple!
<code>bash peeking task
redis 127.0.0.1:6379&gt; LRANGE todo:kuba:todolist:1 0 -1
1) "1"
redis 127.0.0.1:6379&gt; GET todo:kuba:task:1
"finish blog post"
redis 127.0.0.1:6379&gt; GET todo:kuba:task:1:created_at
"1343324314"
</code>
Ok so now we have very simple todo lists with tasks, well at least overview. Ofc you can use sets for todo lists or zsets but lets stay with lists to keep it simple for now.</p>

<p>How ro remove task from the list ?
<code>bash removing task
redis 127.0.0.1:6379&gt; LREM todo:kuba:todolist:1 -1 1
(integer) 1
redis 127.0.0.1:6379&gt; LRANGE todo:kuba:todolist:1 0 -1
(empty list or set)
</code>
Good, now we can add tasks, remove tasks, same sotry with adding todo lists and removing todo lists.
One last thing is to add tags!. Simply here each task will have list of tags and each tag will have list of tasks related with.
<code>bash
todo:&lt;user_id&gt;:task:&lt;task_id&gt;:tags =&gt; [LIST]
todo:&lt;user_id&gt;:tag:&lt;tag&gt;:tasks =&gt; [LIST]
</code>
So how we will add tags to tasks ? Simple!
<code>bash tagging
redis 127.0.0.1:6379&gt; LPUSH todo:kuba:task:1:tags "redis"
(integer) 1
redis 127.0.0.1:6379&gt; LPUSH todo:kuba:task:1:tags "design"
(integer) 2
redis 127.0.0.1:6379&gt; LPUSH todo:kuba:tag:redis:tasks 1
(integer) 1
redis 127.0.0.1:6379&gt; LPUSH todo:kuba:tag:design:tasks 1
(integer) 1
redis 127.0.0.1:6379&gt; LRANGE todo:kuba:tag:design:tasks 0 -1
1) "1"
redis 127.0.0.1:6379&gt; LRANGE todo:kuba:task:1:tags 0 -1
1) "design"
2) "redis"
</code>
This example shows what we need to do to tag a task with something and how to peek tasks tagged with it. Why we have both lists ? To make it fast while searching. If user will click on particular tag like "redis" you want to get it O(1) time not O(N) after searching all keys. And same the other way, normal ui will pull task test, when it was created and tags to display so we want to have this data ready.</p>

<p>This is whole design for the todo app. We have 8 types of keys.  Things like pagination, calculating time are all left to app layer. Important thing is that i scope everything to user key / id. This is because i want to isolate each user space easy. Each user in his own space will have short lists, there is no danger of "ultimate"  non splittable lists.</p>

<h1>Design using Mongodb</h1>

<p>Well this case upfront is easier to grasp because for each list we can use single document or collection of documents lets talk about both solutions.</p>

<h2>Todolist = Document</h2>

<p>In this example we will use built in "array" operators</p>

<p>``` javascript  creating</p>

<blockquote><p>db.todolists.save({name: "House work", tasks: []})
"ok"
db.todolists.find({name: "House work"})
[
  {   "name" : "House work",   "_id" : {   "$oid" : "50118742cc93742e0d0b6f7c"   },</p>

<pre><code>  "tasks" : [   ]   }
</code></pre>

<p>]
```</p></blockquote>

<p>So lets add a task :)
``` javascript
db.todolists.update({name: "House work"},{$push:{"tasks":{"name":"finish blog post", "tags":["mongo"] } } })
"ok"</p>

<blockquote><blockquote><p>db.todolists.find({name: "House work"})
[
  {   "name" : "House work",   "_id" : {   "$oid" : "50118742cc93742e0d0b6f7c"   },</p>

<pre><code>  "tasks" : [  
   {   "name" : "finish blog post",   "tags" : [   "mongo" ]   }
  ]   }
</code></pre>

<p>]
```</p></blockquote></blockquote>

<p>this will create empty todo list with name "House work" of course this way we will not omit building sub lists of tags etc, we have to build in a same way like in redis but as part of document. The story is exactly the same like above in redis. Mongodb lets us query nested documents and this will enable us to skip some of the extra "lists" while doing search.</p>

<p>Lets try it out how to find out mongo tagged entries?</p>

<p>``` javascript find by tag</p>

<blockquote><p>db.todolists.find({"tasks.tags": { $in : ["mongo"] } })</p></blockquote>

<p>[
  {   "name" : "House work",   "_id" : {   "$oid" : "50118742cc93742e0d0b6f7c"   },</p>

<pre><code>  "tasks" : [ 
    {   "name" : "finish blog post",   "tags" : [   "mongo" ]   }
  ]   }
</code></pre>

<p>]
```</p>

<p>This way we can find whole todolist with task that contains tag "mongo" but after that we will have to work out from the document  in app the task that we are interested int. Using it like this we will have a document with structure like this
``` javascript  todo list structure
{
 user: "user id",
 name: "<name>",
 tasks: [
   {</p>

<pre><code>text : "todo text",
tags : [
  "Tag1", "Tag2"
]
created_at : Time,
updated_at : Time
</code></pre>

<p>   }
 ]
}
```
Using redis we could wrapp stuff into MULTI command while using mongodb "array" command we are a bit cowboishing. They could remove wrong stuff in we will not be cautious :). (well same in redis!) Big plus of Mongodb is native time type!</p>

<h2>Todolist = many documents</h2>

<p>Using this approach we can leverage more of our stuff on mongodb search in this approach each task will be a different document.
With structure like this
``` javascript task structure
{
  todo_list: "todo list id",
  user: "user id",
  text: "todo text",
  tags: ["Tag1", "Tag2"]
}</p>

<p><code>
This way we will have a lot of documents, more disk space consumption and still we will have to have second collection with with objects with structure like this
</code> javascript structure of todolist
{
  todo_list: "todo list id",
  name: "todo list name",
  user: "user id",
  // tasks: [Tasks OBjectID Array] you could have this and remove todo_list id from tasks choice is yours :)
}
<code>``
And this way we can use</code>find` tool very easy and get documents fast.</p>

<h1>Summary</h1>

<p>All of this solutions have some pros and cons, mongodb excels better when documents are bigger (limit is set on 16 mb per document) than loads of small documents (massive waste of space). Solution in redis is really fast and if you will implement lazy loading it will be very fast.  You can adjust this designs to your situation by changing lists to sets etc. The place where redis OWNS mongodb in this context is "strucutres" and we use a lot of them to store data like this, lists sets, zsets. Implementing priority list in mongodb will be totally custom solution while in redis we can just use zset.</p>

<p>This is just my point of view on this. I will supply some code to cover it more in part two. This is next problem, i'm sure solution in mongodb using things like mongoid <a href="http://mongoid.org">http://mongoid.org</a> will be much more developer friendly then building things "rawly" in redis hiredis client.</p>

<p>btw i jsut wrote this from "top of my head" so it may contain typos and i'm sure keys, structures can be optimized :) This is just to open discussion with my friend :)</p>

<p>Cheers!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up replication with Redis]]></title>
    <link href="http://JakubOboza.github.com/blog/2012/04/29/setting-up-replication-with-redis/"/>
    <updated>2012-04-29T20:16:00+01:00</updated>
    <id>http://JakubOboza.github.com/blog/2012/04/29/setting-up-replication-with-redis</id>
    <content type="html"><![CDATA[<p>Everyone who wants to feel safe about his data wants to have some sort of backup :). Redis have a support for replication.
And it is very easy to setup.</p>

<h1>Setup!</h1>

<p>To setup replica node all you have to do is to add one line <code>slaveOf</code> in config @_@ of your new Redis instance.
Sounds easy :). Lets think about most basic scenario.</p>

<p>Two nodes, master node and slave node. For purpose of this example you can just start redis using <code>redis-server</code> command without by default he will start on port <code>6379</code> and this is all we need to know to setup replication.</p>

<h2>Configuration of replica node</h2>

<p>To configure replica node all we need to do is to create place to store the db eg. <code>mkdir replica_db</code> and choose port eg. <code>7789</code>. Last thing to do is to create config and point this node to the master. For me it looks like this:
``` bash redis-replica.conf
daemonize no
timeout 0
loglevel notice
logfile stdout
databases 16
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir ./replica_dir
slave-serve-stale-data yes
slave-read-only yes
appendonly no
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
lua-time-limit 5000</p>

<p>pidfile /var/run/redis-replica-7789.pid
port 7789</p>

<h1>replication config</h1>

<p>slaveof 127.0.0.1 6379</p>

<p>slowlog-log-slower-than 10000
slowlog-max-len 1024
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-entries 512
list-max-ziplist-value 64
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
<code>``
Here the important thing really is</code>slaveof 127.0.0.1 6379<code>where we set where is our master.</code>port 7789<code>important if we are using few redis instances on one box and</code>dir ./replica_dir` be sure to not point this to master node db path if you do... you will suffer eternal flame.</p>

<p>Now just start the node <code>redis-server redis-replica.conf</code> and he will start syncing.</p>

<h1>Checking if everything works</h1>

<p>So by now we should have master running on default port and replica connected to it. Lets connect using <code>redis-cli</code> to master and set some keys eg. <code>set name kuba</code>.  Now lets connect to our replica.  If you followed the same configuration then me you can simply do <code>./redis-cli -p 7789</code> this will prompt you with regular command line interface. Now jsut type <code>get name</code></p>

<p><code>
redis 127.0.0.1:7789&gt; get name
"jakub"
</code>
Bang works!</p>

<h1>RO</h1>

<p>Important information is that one master can have many replicas and each replica is read only! So you can connect to it and read from it if you want / need.</p>

<p><code>bash
 λ ./redis-cli -p 7789
redis 127.0.0.1:7789&gt; keys *
 1) "age"
 2) "name"
redis 127.0.0.1:7789&gt; get name
"jakub"
redis 127.0.0.1:7789&gt; set name "not jakub"
(error) READONLY You can't write against a read only slave.
redis 127.0.0.1:7789&gt;
</code></p>

<h1>Summary</h1>

<p>I never had deadly important data in redis :) But still its worth knowing how to setup this just in case something goes wrong you may want to have replica ready :).</p>

<p>On official site <a href="http://redis.io/topics/replication">http://redis.io/topics/replication</a> you can learn more about replication in redis.</p>

<p>Cheers!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up redis cluster]]></title>
    <link href="http://JakubOboza.github.com/blog/2012/04/16/setting-up-redis-cluster/"/>
    <updated>2012-04-16T19:54:00+01:00</updated>
    <id>http://JakubOboza.github.com/blog/2012/04/16/setting-up-redis-cluster</id>
    <content type="html"><![CDATA[<p>redis cluster in currently unstable, i used todays master HEAD (93a74949d7bb5d0c4115d1bf45f856c368badf31) commit to build my redis server and client. Setting redis cluster requires only few settings to go! :)</p>

<p>Here is link to overview how redis cluster works <a href="http://redis.io/presentation/Redis_Cluster.pdf">http://redis.io/presentation/Redis_Cluster.pdf</a></p>

<h1>redis.conf</h1>

<p>Regular nodes can't be part of cluster :( so you have to prepare separate redis configs for your cluster servers.
Most important thing is to setup <code>cluster-enabled</code> and <code>cluster-config-file</code> I decided to name my config files <code>redis-cluster-&lt;port&gt;.conf</code>. I used ports 4444, 4445 4446
Here is my sample config
``` bash
daemonize yes
timeout 0
loglevel notice
logfile stdout
databases 16
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir ./cluster_4444
slave-serve-stale-data yes
slave-read-only yes
appendonly no
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
lua-time-limit 5000</p>

<h1>Cluster</h1>

<p>#
pidfile /var/run/redis-4444.pid
port 4444
cluster-enabled yes
cluster-config-file redis-cluster-4444.conf</p>

<p>slowlog-log-slower-than 10000
slowlog-max-len 1024
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-entries 512
list-max-ziplist-value 64
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
<code>``
For each node i created directory</code>cluster_<port><code>and that was the hardest part actually to do. With this all you have to do is to start ( for debug you can set daemonize to no) all nodes using</code>redis-server path/to/redis-cluster-<port>.conf` and then use magic ruby tool :)</p>

<h1>redis-tribe.rb</h1>

<p>In <code>src/</code> directory of source you can find ruby script for creating and managing cluster. But first you need to have ruby installed with <code>redis</code> gem. i just did <code>gem install redis</code> but if you don't have ruby you have to google how to install it etc (hint: get 1.9.2).</p>

<p>now you can run the script. <code>./redis-tribe.rb</code> and see</p>

<p>``` bash
λ ./redis-trib.rb   <br/>
Usage: redis-trib <command> <arguments ...></p>

<p>  create               host1:port host2:port ... hostN:port
  check                host:port
  reshard              host:port
```</p>

<p>To start cluster we will type "create" (useless comment)</p>

<p><code>bash
λ ./redis-trib.rb create 127.0.0.1:4444 127.0.0.1:4445 127.0.0.1:4446
Creating cluster
Connecting to node 127.0.0.1:4444: OK
Connecting to node 127.0.0.1:4445: OK
Connecting to node 127.0.0.1:4446: OK
Performing hash slots allocation on 3 nodes...
[FAIL] 5a2f6df453f1cd52bcb22c2afc45580283bcce87 127.0.0.1:4444 slots:0-1364 (1365 slots)
[FAIL] 35d107017bc726ece9b57e1ea2f21678555cf6a8 127.0.0.1:4445 slots:1365-2729 (1365 slots)
[FAIL] 76d06b0d3cb1b3829cb60574260dff2d06964cea 127.0.0.1:4446 slots:2730-4095 (1366 slots)
Can I set the above configuration? (type 'yes' to accept): yes
** Nodes configuration updated
** Sending CLUSTER MEET messages to join the cluster
Performing Cluster Check (using node 127.0.0.1:4444)
[FAIL] 5a2f6df453f1cd52bcb22c2afc45580283bcce87 127.0.0.1:4444 slots:0-1364 (1365 slots)
[FAIL] 35d107017bc726ece9b57e1ea2f21678555cf6a8 127.0.0.1:4445 slots:1365-2729 (1365 slots)
[FAIL] 76d06b0d3cb1b3829cb60574260dff2d06964cea 127.0.0.1:4446 slots:2730-4095 (1366 slots)
[OK] All 4096 slots covered.
</code>
Nice we have our cluster running :) now we can connect to any node and try it out.</p>

<p><code>bash
λ ./redis-cli -h 127.0.0.1 -p 4445
redis 127.0.0.1:4445&gt; set "jakub" "oboza"
(error) MOVED 198 127.0.0.1:4444
</code></p>

<p>Sweet :D</p>

<p>Using this tool you can also reshard :D I did on my 15 keys worked :-F.</p>

<h1>Smart clients</h1>

<p>In redis doc we can read that you will require "smart client" to make it low latency. Yes, you can read from output that it was moved so you will have to cache where the key is now and reset temp cache when it will be moved (resharding)</p>

<h1>Fire!</h1>

<p>You can now test how it will behaves under fire by killing and restarting your nodes eg.
<code>bash
[19008] 16 Apr 19:44:09.945 # Server started, Redis version 2.9.7
[19008] 16 Apr 19:44:09.946 * The server is now ready to accept connections on port 4444
[19008] 16 Apr 19:45:14.414 * Connecting with Node c20290a7b70a2a840a168c3309f00e3de1b1844d at 127.0.0.1:14446
[19008] 16 Apr 19:45:15.424 * Connecting with Node ab93647957ed4bb93fc43b1dc76202a6cdb94f49 at 127.0.0.1:14445
[19008] 16 Apr 19:59:10.047 * 1 changes in 900 seconds. Saving...
[19008] 16 Apr 19:59:10.047 * Background saving started by pid 19321
[19321] 16 Apr 19:59:10.080 * DB saved on disk
[19008] 16 Apr 19:59:10.248 * Background saving terminated with success
[19008] 16 Apr 20:08:29.837 * I/O error reading from node link: connection closed
[19008] 16 Apr 20:08:29.837 * I/O error reading from node link: connection closed
[19008] 16 Apr 20:08:30.056 * Connecting with Node 76d06b0d3cb1b3829cb60574260dff2d06964cea at 127.0.0.1:14446
[19008] 16 Apr 20:08:30.056 * I/O error writing to node link: Broken pipe
[19008] 16 Apr 20:08:30.525 * I/O error reading from node link: connection closed
[19008] 16 Apr 20:08:30.526 * I/O error reading from node link: connection closed
[19008] 16 Apr 20:08:31.063 * Connecting with Node 35d107017bc726ece9b57e1ea2f21678555cf6a8 at 127.0.0.1:14445
[19008] 16 Apr 20:08:31.064 * Connecting with Node 76d06b0d3cb1b3829cb60574260dff2d06964cea at 127.0.0.1:14446
</code></p>

<h1>Summary</h1>

<p>Even if i think this is a great tool, and is unstable i saw after few minutes play that some things just don't work as intended and some keys are not pushed. But it is pulled from unstable branch so i'm crossing my fingers for this project because it looks sweet! Go Go Antirez.</p>
]]></content>
  </entry>
  
</feed>
